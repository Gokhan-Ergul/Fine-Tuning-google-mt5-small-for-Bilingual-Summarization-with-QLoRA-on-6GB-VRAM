{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa537e6e-df26-4d1f-bbe5-8040a75c98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Important note: datasets version 3.6.0 will work if you got an error like: runtimeerror: dataset scripts are no longer supported, but found amazon_reviews_multi.py\n",
    "\n",
    "en_ds = load_dataset(\"abisee/cnn_dailymail\", \"2.0.0\")\n",
    "\n",
    "tr_ds = load_dataset(\"reciTAL/mlsum\", \"tu\") # 'tu' is the code for Turkish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16bbb3c9-311e-4d38-a90c-41cec71b1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> text: {example['text']}'\")\n",
    "        print(f\"'>> summary: {example['summary']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f65da15-e98a-4206-b1b4-621629386a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
       "        num_rows: 249277\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
       "        num_rows: 11565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary', 'topic', 'url', 'title', 'date'],\n",
       "        num_rows: 12775\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81785aa-66ee-45c0-a391-e18a9cc5e7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 249277\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 11565\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 12775\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_ds = tr_ds.select_columns(['text', 'summary'])\n",
    "tr_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c549e3bc-cabd-4aa9-9f70-75e3bdb831b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4108d133-778e-48df-9249-6244db46d1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'summary'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_ds = en_ds.select_columns([\"article\",\"highlights\"])\n",
    "column_mapping = {\n",
    "    \"article\": \"text\",\n",
    "    \"highlights\": \"summary\"\n",
    "}\n",
    "en_ds = en_ds.rename_columns(column_mapping)\n",
    "en_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafdb077-4b31-4319-b023-7370535d592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_ds = en_ds.filter(lambda x: len(x['summary'].split()) > 5)\n",
    "tr_ds = tr_ds.filter(lambda x: len(x['summary'].split()) > 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35af6eb5-534d-48a7-b71f-8818455707df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae703071-0e91-4126-8d8f-a8d843f1d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 30 #i should check this too, because probably most of the labels didin't stop i just cut them.\n",
    "\n",
    "def filter_long_inputs(example):\n",
    "    input_length = len(tokenizer(example[\"text\"], truncation=False)[\"input_ids\"])\n",
    "    return input_length <= max_input_length\n",
    "\n",
    "filtered_ds_en = en_ds.filter(filter_long_inputs)\n",
    "filtered_ds_tr = tr_ds.filter(filter_long_inputs)\n",
    "original_columns = filtered_ds_tr['train'].column_names\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True, #this is be sure that there is no any row contain more then 512 tokens\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"summary\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "filtered_ds_en = filtered_ds_en.map(preprocess_function, batched=True, remove_columns=original_columns)\n",
    "filtered_ds_tr = filtered_ds_tr.map(preprocess_function, batched=True,remove_columns=original_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1c178e-190c-4e40-abc9-f900cf97db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "#filtered_ds_en.save_to_disk(\"tokenized_english_dataset\")\n",
    "#filtered_ds_tr.save_to_disk(\"tokenized_turkish_dataset\")\n",
    "\n",
    "filtered_ds_en = load_from_disk(\"tokenized_english_dataset\")\n",
    "filtered_ds_tr = load_from_disk(\"tokenized_turkish_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41ee9bc8-b6c2-4ed8-9d87-0ceecaf0db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 152689\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 8133\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9134\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30846006-78e9-4274-96d8-3b9f55db3876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 33189\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1952\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef89bc2-2bc5-41e4-93a6-bea9f7625f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ds_tr['train'] = filtered_ds_tr['train'].shuffle(seed=42).select(range(33189))\n",
    "filtered_ds_tr['validation'] = filtered_ds_tr['validation'].shuffle(seed=42).select(range(1952))\n",
    "filtered_ds_tr['test'] = filtered_ds_tr['test'].shuffle(seed=42).select(range(1574))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d9f771-429f-4bd6-8af6-91861b71797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "total_ds = DatasetDict()\n",
    "for split in filtered_ds_tr.keys():\n",
    "    total_ds[split] = concatenate_datasets(\n",
    "        [filtered_ds_tr[split], filtered_ds_en[split]]\n",
    "    )\n",
    "    total_ds[split] = total_ds[split].shuffle(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c93785-7179-4898-a951-e15f5008cf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 66378\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3904\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8338d5-58e3-47f9-8f67-d6f26deb8710",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "493019b9-a0af-48a7-be27-379379fe5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 688,128 || all params: 300,864,896 || trainable%: 0.2287\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_id = \"google/mt5-small\"\n",
    "\n",
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",             # 4-bit NormalFloat\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for training to calculate weights but don't use fp16= True on args with this.\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model in 4-bit\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # This will automatically put the model on your RTX 3050\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # The \"rank\" of the new adapters (higher r = more params, 8 or 16 is a good start)\n",
    "    lora_alpha=32,  # A scaling factor (often 2x r)\n",
    "    # For mt5, we target the query (\"q\") and value (\"v\") layers in the attention blocks\n",
    "    target_modules=[\"q\", \"v\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"  # This is CRITICAL for T5/MT5 models\n",
    ")\n",
    "# This wraps your frozen, quantized model with the new, trainable LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8610722-9e82-436d-ab4b-c827b45abd4a",
   "metadata": {},
   "source": [
    "`Recall` measures what percentage of the words in the human-written reference summary the model can capture. It looks at how much the reference summary \"covers.\"\n",
    "\n",
    "`Precision` measures how accurately the model extracts some of the words from its own context (i.e., the reference summary is also included). It penalizes how much \"nonsense\" or unnecessary words the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256eef68-0f9b-46b9-9186-2d0747d1d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122a4250-f033-4054-82fb-6cd0fbd1361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summary =  \"The flight is delayed due to bad weather.\"\n",
    "generated_summary = \"The flight is canceled due to weather.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be4171b-6a6a-424a-85f4-acde36a9f291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.7999999999999999,\n",
       " 'rouge2': 0.4615384615384615,\n",
       " 'rougeL': 0.7999999999999999,\n",
       " 'rougeLsum': 0.7999999999999999}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472593c5-5eef-4945-ab0f-da35f02957fb",
   "metadata": {},
   "source": [
    "* rouge1: Unigram Overlap ---> This score looks at the overlap of individual words (unigrams) between your generated summary and the reference summary.\n",
    "* rouge2: Bigram Overlap ---> This score looks at the overlap of pairs of adjacent words (bigrams). It's a better measure of fluency and sentence structure than rouge1\n",
    "* rougeL: Longest Common Subsequence ---> This score finds the Longest Common Subsequence (LCS) between the two summaries. An LCS is the longest sequence of words that appears in both summaries in the same order, but not necessarily right next to each other\n",
    "* rougeLsum: Summary-Level LCS ---> This is the same as rougeL but applied at the summary level (it looks for the LCS across the entire text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99cdfe06-e237-4ea2-a9d3-3f46a291db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5fef413-63f1-4310-b34c-728376d312b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.923076923076923,\n",
       " 'rouge2': 0.7272727272727272,\n",
       " 'rougeL': 0.923076923076923,\n",
       " 'rougeLsum': 0.923076923076923}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e85d3128-6781-4e04-8d50-a617612745c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gokhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cecf41c2-5e60-40c8-be6f-613872d84825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    #predictions = np.argmax(predictions, axis=-1) we already use predict_with_generate \n",
    "\n",
    "    if torch.is_tensor(predictions):\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    if torch.is_tensor(labels):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "\n",
    "    predictions = np.nan_to_num(predictions, nan=tokenizer.pad_token_id, # on train mode it turning nan, inf values we avoid them with this lines.\n",
    "                                 posinf=tokenizer.pad_token_id, \n",
    "                                 neginf=tokenizer.pad_token_id)\n",
    "    \n",
    "    labels = np.nan_to_num(labels, nan=tokenizer.pad_token_id) \n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    predictions = predictions.astype(np.int64) #It guaranteed that we would get an array of type int (integer), which is exactly the format it expected, and it resolved the OverflowError error once and for all.\n",
    "    labels = labels.astype(np.int64)\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True) #i got error for this line.\n",
    "    #the error line was: OverflowError: out of range integral type conversion attempted\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    decoded_preds_nltk = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels_nltk = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds_nltk,\n",
    "                                      references=decoded_labels_nltk,\n",
    "                                      use_stemmer=True)\n",
    "    \n",
    "    bert_result = bertscore_metric.compute(predictions=decoded_preds,\n",
    "                                         references=decoded_labels,\n",
    "                                         model_type=\"bert-base-multilingual-cased\",\n",
    "                                         device=\"cpu\")\n",
    "\n",
    "    \n",
    "    result = {k: v for k, v in rouge_result.items()}\n",
    "\n",
    "    result[\"bertscore_f1\"] = np.mean(bert_result[\"f1\"])\n",
    "    \n",
    "    # Add generation length to see if your model is too verbose/short\n",
    "    gen_len = np.mean([np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions])\n",
    "    result[\"gen_len\"] = gen_len\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92538426-070b-47a5-adc1-0055e56979bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing compute_metrics function...\n",
      "Caching BERTScore model (downloading if necessary)...\n",
      "BERTScore modeli hazır.\n",
      "\n",
      "--- Test Data (Token IDs) ---\n",
      "Mock Predictions (contains NaN/Inf):\n",
      " [[4.17530e+04 1.14120e+04 2.17630e+04 2.77000e+02 1.53624e+05 2.60000e+02\n",
      "  1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00         inf\n",
      "          nan 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [1.15300e+03 2.59000e+02 6.73700e+03 5.03830e+04 6.98000e+02 3.17000e+02\n",
      "  1.73770e+05 2.16700e+03 2.60000e+02 1.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00\n",
      "  0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]]\n",
      "\n",
      "Mock Labels (contains -100):\n",
      " [[  9275  36513    259 123182  62717  20108  21763    277    407  13192\n",
      "   18202    260      1   -100   -100   -100   -100   -100   -100   -100\n",
      "    -100   -100   -100   -100   -100   -100   -100   -100   -100   -100]\n",
      " [   486  10292  31533    351    287   3071    260      1   -100   -100\n",
      "    -100   -100   -100   -100   -100   -100   -100   -100   -100   -100\n",
      "    -100   -100   -100   -100   -100   -100   -100   -100   -100   -100]\n",
      " [  1153    259   6737  50383    698 203250    259  72495   1838   6900\n",
      "     260      1   -100   -100   -100   -100   -100   -100   -100   -100\n",
      "    -100   -100   -100   -100   -100   -100   -100   -100   -100   -100]]\n",
      "\n",
      "--- compute_metrics calling ---\n",
      "\n",
      "--- Succesfull ---\n",
      "calculated matrics:\n",
      "{'rouge1': 0.3313, 'rouge2': 0.1481, 'rougeL': 0.3313, 'rougeLsum': 0.3313, 'bertscore_f1': 0.5717, 'gen_len': 6.0}\n",
      "\n",
      " all of the metrics are exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "print(\"testing compute_metrics function...\")\n",
    "\n",
    "try:\n",
    "    print(\"Caching BERTScore model (downloading if necessary)...\")\n",
    "    bertscore_metric.compute(predictions=[\"test\"], references=[\"test\"], \n",
    "                             model_type=\"bert-base-multilingual-cased\", device=\"cpu\")\n",
    "    print(\"BERTScore modeli hazır.\")\n",
    "except Exception as e:\n",
    "    print(f\"BERTScore model download failed, metric will return 0. Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "mock_labels_text = [\n",
    "    \"Türkiye Büyük Millet Meclisi Ankara'da bulunmaktadır.\", # Turkish referans\n",
    "    \"The cat sat on the mat.\", # english referans\n",
    "    \"Bu üçüncü bir referans cümlesidir.\"\n",
    "]\n",
    "\n",
    "mock_preds_text = [\n",
    "    \"TBMM Ankara'dadır.\", # best preds\n",
    "    \"\",\n",
    "    \"Bu üçüncü bir cümledir.\"\n",
    "]\n",
    "\n",
    "max_len = 30 \n",
    "\n",
    "labels_tokenized = tokenizer(mock_labels_text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "mock_labels_ids = np.array(labels_tokenized['input_ids'])\n",
    "mock_labels_ids[mock_labels_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "preds_tokenized = tokenizer(mock_preds_text, max_length=max_len, padding=\"max_length\", truncation=True)\n",
    "mock_preds_ids = np.array(preds_tokenized['input_ids'], dtype=float) \n",
    "\n",
    "# OverflowError'u testing \n",
    "mock_preds_ids[1, 5] = np.inf \n",
    "mock_preds_ids[1, 6] = np.nan\n",
    "\n",
    "print(\"\\n--- Test Data (Token IDs) ---\")\n",
    "print(\"Mock Predictions (contains NaN/Inf):\\n\", mock_preds_ids)\n",
    "print(\"\\nMock Labels (contains -100):\\n\", mock_labels_ids)\n",
    "\n",
    "\n",
    "mock_eval_preds = (mock_preds_ids, mock_labels_ids)\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- compute_metrics calling ---\")\n",
    "    metrics = compute_metrics(mock_eval_preds)\n",
    "    print(\"\\n--- Succesfull ---\")\n",
    "    print(\"calculated matrics:\")\n",
    "    print(metrics)\n",
    "\n",
    "    # Sonuçları Doğrula\n",
    "    assert \"rouge1\" in metrics\n",
    "    assert \"bertscore_f1\" in metrics\n",
    "    assert \"gen_len\" in metrics\n",
    "    assert metrics[\"gen_len\"] > 0 # gen_len 0 olmamalı\n",
    "    print(\"\\n all of the metrics are exist.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n--- fail  ---\")\n",
    "    print(f\"An error occurred while the function was running: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc9839a0-ffad-42eb-b812-30e9d846c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Name: gokhanErgul\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login,whoami\n",
    "\n",
    "login(token=\"hf_XXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(\"User Name:\", user_info['name'])\n",
    "except Exception as e:\n",
    "    print(\" Failed to log in\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce086ba-1caf-459b-840c-9535c3e74457",
   "metadata": {},
   "source": [
    "What was the Issue? The error was caused by the fp16=True setting in Seq2SeqTrainingArguments. This setting forced LoRA adapters and gradient calculations to be in the unstable float16 format, which resulted in a NaN/OverflowError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5725593b-f656-4975-bd73-14a602053afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# This is the batch size for one device (your GPU)\n",
    "per_device_batch_size = 8 \n",
    "# This is the number of steps to accumulate gradients for\n",
    "# This gives you a larger *effective* batch size\n",
    "gradient_accumulation_steps = 8\n",
    "# This is your true, effective batch size\n",
    "batch_size = per_device_batch_size * gradient_accumulation_steps\n",
    "\n",
    "logging_steps = len(total_ds[\"train\"]) // batch_size\n",
    "logging_steps = 250\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir= f\"{model_name}-finetuned-tr-en\",\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=5.6e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8, # it was 3 before but i didn't like the performance.\n",
    "    logging_steps=logging_steps,\n",
    "    max_grad_norm=1.0, #this will ensure that there is no nan. The magnitude of the learning signal (gradient) can never exceed 1.0.\"\n",
    "    \n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    \n",
    "    per_device_eval_batch_size=16,\n",
    "    \n",
    "    #fp16=True, this caused to nan values at validation loss on traning \n",
    "    bf16=True,\n",
    "    \n",
    "    dataloader_num_workers=8,\n",
    "    \n",
    "    # torch_compile=True,  it doesn't work on me.\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False, #i trained it withoud internet.\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19c9cd0b-41d7-40f8-989e-085c876d5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fe4c54a-99a8-4404-9da6-006622309b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  1641,    715,  71213,    276,    261,  36592,  29209,   2186,   5061,\n",
       "          45215,   5522,  26618,  56304, 105132,    261,    313,  84301,    270,\n",
       "          88025,  57990,  60811,  46829,   1099,   3436,  26584,    314,    261,\n",
       "            390,   7942,    529,  33564, 114716,    263,    620,  67027,   2222,\n",
       "          66716,  87181, 127915, 196210,  15109,    648,    529,  87202,    259,\n",
       "          76323,   2172,    261,  89721,    259,  39557,   6400,   2346,  90749,\n",
       "         213212,    259,  90008,    293,   2667,  54006, 200473,    330,  40357,\n",
       "           2941,  28686,    293,    407, 153062,    314,    260,    419,    259,\n",
       "            318,  96264,  75060,    272,   6789, 105132,   3143,   8377,  22796,\n",
       "          34059,    259,  76323,  18434,    261,  46386,  23468,    698,   1099,\n",
       "           9437,  57938,    259,   1986,  69277,    529,  24111,  41798,   2222,\n",
       "          15846,    266,  43600,    266,  43780, 224921,  40373,  42278, 105612,\n",
       "          11454,    261,    259,    277,    490, 208612,    268,  15816,   8669,\n",
       "            259,   2667,  39602,  36096,   1137,    569,    261,   4158,    269,\n",
       "          17610,   5814,  43844,    286,    529,  24111,  65579,   1714,   7409,\n",
       "            494,    435,  23177,  30204, 101543,    277,    269,  88983,    260,\n",
       "          11992,   1714,    368,    528,    270,  46257,    311,    259,  54927,\n",
       "         136823, 126185,    260,      1,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0],\n",
       "        [   259, 110716,    277,   5642,   4418,    265,    276, 152890,   7264,\n",
       "          14094,  33156,    273, 113447,  42342,   8238,    261,   1508,   6157,\n",
       "           3436,    261,  12626,  43064,   4407,   2059,  58414,    321,    698,\n",
       "          10462,   2186,    344,  32177,  25637,   8543,  91851,    278,   7625,\n",
       "         107510,  71248,  18864,  56609,    260,  35439,  16110,   7567,    753,\n",
       "          13927,    261,  31464, 177150,   3724,  71248,  29328,  12626,  43064,\n",
       "            262,  16060,    321,    529,   1287,    266,  69010,    325,  11554,\n",
       "            494,    259,    268, 139652,    259,  65138,   1647,    261,  11639,\n",
       "         111354,    272,    330,  86141,   5749,   5225,    344, 130193,    261,\n",
       "          58414,    648,  34947, 194078,  13192,    321,   1213,  22796,  34059,\n",
       "          19379, 106558,    285,   1946,    339,  31396,  10462,   2186,    261,\n",
       "           4331, 214902, 193408,   8599, 130102,   1714,  91851,    278,    432,\n",
       "          20449,  42488,  71248,  18864,   4913,    377,  18639,    314, 197178,\n",
       "            260,  11039,  63261,    261,   7263,    462,  20449,  42488,   8954,\n",
       "          16941,    259,  84628,   3817,    326, 113113,   2407,    473,   8669,\n",
       "          28350,  73416,    259,  53801,   4460, 129446,    405,  86632,    260,\n",
       "            852, 191550,  41092,   7625, 107510,    261,   4331, 179186,   5372,\n",
       "            261,  58414,    321,   9822,   1785,   1099, 164377,    344,  32177,\n",
       "          25637,  87181,    259,  54496,    259, 237924,    260,  11039,    753,\n",
       "          58105,    432,  20449,  25179,  71248,  18864,    330,  97710,    261,\n",
       "          22973,  10863, 148902,    278,  10462,   2432,  30388,   9124,  90869,\n",
       "            400,   2086, 176302,  14178,  25637,  14880,    260,    412,  30817,\n",
       "           2432,  30388,   1954,    261,  26992,   2432, 164113,  78807,    259,\n",
       "           3588,   5225,   7567,  63261,  15816,   7824,    261,  35439,  16110,\n",
       "            259, 132930,  23417,   1281,    276, 191785,    259,  79356,    845,\n",
       "            344,   8543,  36607,    650,  11645,  74785, 144231,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ 71671,  60549,  82965,   1641,    715,  71213,    276,    261,  60549,\n",
       "          12812,  46182,    259,  59305, 146226,    698,  12027,  20297,    260,\n",
       "            419,    259,    318,  96264,  75060,    272,   6789, 105132,   3143,\n",
       "          89721,    259,      1],\n",
       "        [ 10462,   1954,  56557,    285,  18831,    383, 174149,   4418,  42775,\n",
       "          21696,    259, 110716,    277,    407,    698,  34159, 108403,   4106,\n",
       "          16065,    739,  60753,    302,      1,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100]]), 'decoder_input_ids': tensor([[     0,  71671,  60549,  82965,   1641,    715,  71213,    276,    261,\n",
       "          60549,  12812,  46182,    259,  59305, 146226,    698,  12027,  20297,\n",
       "            260,    419,    259,    318,  96264,  75060,    272,   6789, 105132,\n",
       "           3143,  89721,    259],\n",
       "        [     0,  10462,   1954,  56557,    285,  18831,    383, 174149,   4418,\n",
       "          42775,  21696,    259, 110716,    277,    407,    698,  34159, 108403,\n",
       "           4106,  16065,    739,  60753,    302,      1,      0,      0,      0,\n",
       "              0,      0,      0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [total_ds[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac5865c-4913-4bc4-931e-e6d256857d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_size = 1600  # 1600 / 8 = 200 train steps\n",
    "val_test_subset_size = 320 # 320 / 16 = 20 eval steps\n",
    "\n",
    "debug_ds = DatasetDict({\n",
    "    \"train\": total_ds[\"train\"].select(range(train_subset_size)),\n",
    "    \"validation\": total_ds[\"validation\"].select(range(val_test_subset_size)),\n",
    "    \"test\": total_ds[\"test\"].select(range(val_test_subset_size))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929fc739-fb47-46cc-adf3-ba8cbdf6b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./debug_run\",          \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_strategy=\"steps\",               \n",
    "    learning_rate=5.6e-5,            \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=16,   \n",
    "    gradient_accumulation_steps=8,   \n",
    "    bf16=True,                       \n",
    "    num_train_epochs=2,              \n",
    "    \n",
    "    logging_steps=10,                  \n",
    "    \n",
    "    predict_with_generate=True, \n",
    "    \n",
    "    push_to_hub=False                  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1de3ee-a25b-4c2a-8b3a-0ce0eb29deb4",
   "metadata": {},
   "source": [
    "Issue: When the model is loaded with QLoRA (4-bit quantization) and the fp16=True setting is used in Seq2SeqTrainingArguments, the Validation Loss appears as nan (Not a Number), and training crashes.\n",
    "\n",
    "Cause: Training a model reduced to 4-bit with QLoRA using the fp16 (float16) format results in numerical instability. The dynamic range of fp16 is not sufficient to handle the very small or very large gradient values ​​generated during calculations with 4-bit weights.\n",
    "\n",
    "This results in nan or inf (infinite) values ​​in the calculations, rendering the model loss mathematically undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18decae5-b765-4b5b-9ffd-e06f10229f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3292/4048526368.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgokhannergull\u001b[0m (\u001b[33mgokhannergull-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gokhan/hugging_face/Summarization /wandb/run-20251021_131609-qrjwqls7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gokhannergull-student/huggingface/runs/qrjwqls7' target=\"_blank\">classic-breeze-21</a></strong> to <a href='https://wandb.ai/gokhannergull-student/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gokhannergull-student/huggingface' target=\"_blank\">https://wandb.ai/gokhannergull-student/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gokhannergull-student/huggingface/runs/qrjwqls7' target=\"_blank\">https://wandb.ai/gokhannergull-student/huggingface/runs/qrjwqls7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>23.752000</td>\n",
       "      <td>13.589259</td>\n",
       "      <td>0.027900</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>5.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>23.405900</td>\n",
       "      <td>13.356882</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>5.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>23.666900</td>\n",
       "      <td>13.126556</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.568100</td>\n",
       "      <td>6.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>23.903700</td>\n",
       "      <td>12.920822</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>5.971900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>22.666200</td>\n",
       "      <td>12.978966</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.568200</td>\n",
       "      <td>6.003100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=23.478935546875, metrics={'train_runtime': 292.6429, 'train_samples_per_second': 10.935, 'train_steps_per_second': 0.171, 'total_flos': 1581293418872832.0, 'train_loss': 23.478935546875, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args= debug_args,\n",
    "    train_dataset=debug_ds['train'],\n",
    "    eval_dataset=debug_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3ea1f54-7d52-4c81-b498-e7ac0832d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting STRESS TEST with real 'validation' data...\n",
      "Successfully received 16 rows of real data and moved it to the GPU.\n",
      "Running model.generate() (This will occur if there is instability)...\n",
      "model.generate() done.\n",
      "\n",
      "--- CALLING compute_metrics (With Actual Model Output)) ---\n",
      "\n",
      "--- STRESS TEST succesfull ---\n",
      "calculated metrics:\n",
      "{'rouge1': 0.032, 'rouge2': 0.0089, 'rougeL': 0.0282, 'rougeLsum': 0.0323, 'bertscore_f1': 0.5826, 'gen_len': 11.5625}\n",
      "If you got here without getting an 'OverflowError',\n",
      "Your compute_metrics function is the model's output (including inf/nan)\n",
      "100% robust against all unstable outputs.\n",
      "It's normal for ROUGE scores to be very low (model untrained).\n",
      "It's also normal for gen_len to be ~10.0 (we forced it with min_length=10).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "print(\"Starting STRESS TEST with real 'validation' data...\")\n",
    "\n",
    "try:\n",
    "    batch_size = 16\n",
    "    eval_sample = total_ds[\"validation\"].select(range(batch_size))\n",
    "    \n",
    "    collated_batch = data_collator(eval_sample)\n",
    "    \n",
    "    device = model.device\n",
    "    input_ids = collated_batch[\"input_ids\"].to(device)\n",
    "    attention_mask = collated_batch[\"attention_mask\"].to(device)\n",
    "    labels = collated_batch[\"labels\"].to(device)\n",
    "    \n",
    "    print(f\"Successfully received {batch_size} rows of real data and moved it to the GPU.\")\n",
    "\n",
    " \n",
    "    print(\"Running model.generate() (This will occur if there is instability)...\")\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            min_length=10, \n",
    "            max_length=30\n",
    "        )\n",
    "    \n",
    "    print(\"model.generate() done.\")\n",
    "\n",
    "    predictions_np = generated_ids.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "\n",
    "    eval_preds = (predictions_np, labels_np)\n",
    "\n",
    "    print(\"\\n--- CALLING compute_metrics (With Actual Model Output)) ---\")\n",
    "    metrics = compute_metrics(eval_preds)\n",
    "    \n",
    "    print(\"\\n--- STRESS TEST succesfull ---\")\n",
    "    print(\"calculated metrics:\")\n",
    "    print(metrics)\n",
    "    \n",
    "    print(f\"If you got here without getting an 'OverflowError',\")\n",
    "    print(f\"Your compute_metrics function is the model's output (including inf/nan)\")\n",
    "    print(f\"100% robust against all unstable outputs.\")\n",
    "    print(f\"It's normal for ROUGE scores to be very low (model untrained).\")\n",
    "    print(f\"It's also normal for gen_len to be ~10.0 (we forced it with min_length=10).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n--- STRESS TEST fail ---\")\n",
    "    print(f\"An error occurred during testing: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc2f5fb5-be4c-4635-8aa9-633dd847ec54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3114' max='3114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3114/3114 06:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3114, training_loss=0.1522744631415081, metrics={'train_runtime': 383.1088, 'train_samples_per_second': 519.784, 'train_steps_per_second': 8.128, 'total_flos': 9.89673909037056e+16, 'train_loss': 0.1522744631415081, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args= args,\n",
    "    train_dataset=total_ds['train'],\n",
    "    eval_dataset=total_ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "trainer.train(resume_from_checkpoint = \"mt5-small-finetuned-tr-en/checkpoint-3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24fffd52-c826-4e00-a4ae-5d64e92cfd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 04:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.309802293777466,\n",
       " 'eval_rouge1': 0.2514,\n",
       " 'eval_rouge2': 0.1323,\n",
       " 'eval_rougeL': 0.2171,\n",
       " 'eval_rougeLsum': 0.2322,\n",
       " 'eval_bertscore_f1': 0.6773,\n",
       " 'eval_gen_len': 19.8591,\n",
       " 'eval_runtime': 387.8001,\n",
       " 'eval_samples_per_second': 10.067,\n",
       " 'eval_steps_per_second': 0.629,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f05c32d5-f2a7-4fdd-84e7-1c6fd3188986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# i am gonna run the model withoud internet.\n",
    "evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "try:\n",
    "    bertscore_metric.compute(\n",
    "        predictions=[\"hello\"], \n",
    "        references=[\"world\"], \n",
    "        model_type=\"bert-base-multilingual-cased\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "389e1cac-4c2b-4cfc-9c30-4a0778d99206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4d09362-0f32-41a1-a4a1-b1e298541965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8304/8304 6:38:42, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Bertscore F1</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>4.147800</td>\n",
       "      <td>3.298348</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.219000</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>19.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.114700</td>\n",
       "      <td>3.278165</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.676300</td>\n",
       "      <td>19.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>4.067200</td>\n",
       "      <td>3.263816</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.134900</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.235100</td>\n",
       "      <td>0.678200</td>\n",
       "      <td>19.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.047600</td>\n",
       "      <td>3.251562</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>4.026600</td>\n",
       "      <td>3.252216</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>19.806400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.022800</td>\n",
       "      <td>3.225316</td>\n",
       "      <td>0.257100</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.238200</td>\n",
       "      <td>0.678900</td>\n",
       "      <td>19.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>4.006600</td>\n",
       "      <td>3.218869</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.219700</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>19.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.003300</td>\n",
       "      <td>3.210943</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.134800</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>19.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>3.992400</td>\n",
       "      <td>3.195982</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.235200</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>19.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.981800</td>\n",
       "      <td>3.208672</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.223700</td>\n",
       "      <td>0.237100</td>\n",
       "      <td>0.678800</td>\n",
       "      <td>19.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>3.964300</td>\n",
       "      <td>3.200877</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.946100</td>\n",
       "      <td>3.202784</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.678400</td>\n",
       "      <td>19.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>3.193174</td>\n",
       "      <td>0.254800</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.955400</td>\n",
       "      <td>3.178493</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>19.668800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>3.948900</td>\n",
       "      <td>3.171056</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.928500</td>\n",
       "      <td>3.170683</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.235300</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>3.917200</td>\n",
       "      <td>3.172897</td>\n",
       "      <td>0.254600</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.926600</td>\n",
       "      <td>3.174152</td>\n",
       "      <td>0.255100</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.223800</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.678700</td>\n",
       "      <td>19.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>3.944300</td>\n",
       "      <td>3.163703</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.222900</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.678400</td>\n",
       "      <td>19.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.911100</td>\n",
       "      <td>3.169718</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>19.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>3.927600</td>\n",
       "      <td>3.168232</td>\n",
       "      <td>0.254300</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.223100</td>\n",
       "      <td>0.235900</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>19.684200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/utils/save_and_load.py:286: UserWarning: Could not find a config file in google/mt5-small - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8304, training_loss=2.546525927637354, metrics={'train_runtime': 23927.9026, 'train_samples_per_second': 22.193, 'train_steps_per_second': 0.347, 'total_flos': 2.6389096594855526e+17, 'train_loss': 2.546525927637354, 'epoch': 8.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args= args,\n",
    "    train_dataset=total_ds['train'],\n",
    "    eval_dataset=total_ds[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "trainer.train(resume_from_checkpoint = \"mt5-small-finetuned-tr-en/checkpoint-3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e0bc3e-ada6-462f-aa63-5446a52c804d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MT5ForConditionalGeneration(\n",
       "  (shared): Embedding(250112, 512)\n",
       "  (encoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): MT5Stack(\n",
       "    (embed_tokens): Embedding(250112, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x MT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): MT5LayerSelfAttention(\n",
       "            (SelfAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): MT5LayerCrossAttention(\n",
       "            (EncDecAttention): MT5Attention(\n",
       "              (q): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear4bit(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear4bit(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): MT5LayerFF(\n",
       "            (DenseReluDense): MT5DenseGatedActDense(\n",
       "              (wi_0): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): MT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): MT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"google/mt5-small\"\n",
    "adapter_path = \"mt5-small-finetuned-tr-en/checkpoint-8304\" \n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "model = model.merge_and_unload() \n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a538976a-e82a-42a0-b8d6-f14ab44e015a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd67c960fa24b819a473260485e729f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1d837d9fd3469ab854f1787d1ba1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gokhanErgul/mt5-small-finetuned-tr-en/commit/d7315a8379f279fcef0e1af206fe6a0733b7df78', commit_message='Upload MT5ForConditionalGeneration', commit_description='', oid='d7315a8379f279fcef0e1af206fe6a0733b7df78', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gokhanErgul/mt5-small-finetuned-tr-en', endpoint='https://huggingface.co', repo_type='model', repo_id='gokhanErgul/mt5-small-finetuned-tr-en'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('gokhanErgul/mt5-small-finetuned-tr-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e44f9398-4078-4e6c-ba42-2760828f508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78f87a313f44282aebb8f26793987f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3ab9a0d9bd4f658316508d7400a07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adaff8945624cf1956b7d72443385cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/gokhanErgul/mt5-small-finetuned-tr-en/commit/56589b7288945821740ba6d2ea842f8b993b364f', commit_message='Upload tokenizer', commit_description='', oid='56589b7288945821740ba6d2ea842f8b993b364f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/gokhanErgul/mt5-small-finetuned-tr-en', endpoint='https://huggingface.co', repo_type='model', repo_id='gokhanErgul/mt5-small-finetuned-tr-en'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('gokhanErgul/mt5-small-finetuned-tr-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38c7e6f3-8e6d-4ff5-b771-e45828d77ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa213567-5bd9-4e77-8d67-987428921da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tr = \"\"\"\n",
    "Türkiye, resmî adıyla Türkiye Cumhuriyeti, topraklarının büyük bölümü Batı Asya'da Anadolu'da, diğer bir bölümü ise Güneydoğu Avrupa'nın uzantısı Doğu Trakya'da olan kıtalararası bir ülkedir. Batıda Bulgaristan ve Yunanistan, doğuda Gürcistan, Ermenistan, İran ve Azerbaycan, güneyde ise Irak ve Suriye ile sınır komşusudur. Güneyini Kıbrıs ve Akdeniz, batısını Ege Denizi, kuzeyini ise Karadeniz çevreler. Marmara Denizi ise İstanbul Boğazı ve Çanakkale Boğazı ile birlikte Anadolu'yu Trakya'dan, yani Asya'yı Avrupa'dan ayırır. Resmî olarak laik bir devlet olan Türkiye'de nüfusun çoğunluğu Müslümandır. Ankara, Türkiye'nin başkenti ve ikinci en kalabalık şehri; İstanbul ise, Türkiye'nin en kalabalık şehri, ekonomik merkezi ve aynı zamanda Avrupa'nın en kalabalık şehridir.Türkiye toprakları üzerinde bulunan ilk yerleşmeler Yontma Taş Devri'nde başlar. Doğu Trakya'da Traklar olmak üzere, Hititler, Frigler, Lidyalılar ve Dor istilası sonucu Yunanistan'dan kaçan Akalar tarafından kurulan İyon medeniyeti gibi çeşitli eski Anadolu medeniyetlerinin ardından, Makedonya kralı Büyük İskender'in egemenliğiyle ve fetihleriyle birlikte Helenistik Dönem başladı. Daha sonra, sırasıyla Roma İmparatorluğu ve Anadolu'nun Hristiyanlaştığı Bizans dönemleri yaşandı. Selçuklu Türklerinin 1071 yılında Bizans'a karşı kazandığı Malazgirt Meydan Muharebesi ile Anadolu'daki Bizans üstünlüğü büyük ölçüde kırılarak Anadolu, kısa süre içerisinde Selçuklulara bağlı Türk beyleri tarafından ele geçirildi ve Anadolu toprakları üzerinde İslamlaşma ve Türkleşme faaliyetleri başladı.\n",
    "\"\"\"\n",
    "\n",
    "text_en = \"\"\"\n",
    "Fine-tuning in machine learning is the process of adapting a pre-trained model for specific tasks or use cases. It has become a fundamental deep learning technique, particularly in the training process of foundation models used for generative AI.\n",
    "Fine-tuning could be considered a subset of the broader technique of transfer learning: the practice of leveraging knowledge an existing model has already learned as the starting point for learning new tasks.\n",
    "The intuition behind fine-tuning is that, essentially, it’s easier and cheaper to hone the capabilities of a pre-trained base model that has already acquired broad learnings relevant to the task at hand than it is to train a new model from scratch for that specific purpose. This is especially true for deep learning models with millions or even billions of parameters, like the large language models (LLMs) that have risen to prominence in the field of natural language processing (NLP) or the complex convolutional neural networks (CNNs) and vision transformers (ViTs) used for computer vision tasks like image classification, object detection or image segmentation.By leveraging prior model training through transfer learning, fine-tuning can reduce the amount of expensive computing power and labeled data needed to obtain large models tailored to niche use cases and business needs. For example, fine-tuning can be used to simply adjust the conversational tone of a pre-trained LLM or the illustration style of a pre-trained image generation model; it could also be used to supplement learnings from a model’s original training dataset with proprietary data or specialized, domain-specific knowledge.Fine-tuning thus plays an important role in the real-world application of machine learning models, helping democratize access to and customization of sophisticated models.\n",
    "Conversely, fine-tuning entails techniques to further train a model whose weights have already been updated through prior training. Using the base model’s previous knowledge as a starting point, fine-tuning tailors the model by training it on a smaller, task-specific dataset.While that task-specific dataset could theoretically have been used for the initial training, training a large model from scratch on a small dataset risks overfitting: the model might learn to perform well on the training examples, but generalize poorly to new data. This would render the model ill-suited to its given task and defeat the purpose of model training.Fine-tuning thus provides the best of both worlds: leveraging the broad knowledge and stability gained from pre-training on a massive set of data and honing the model’s understanding of more detailed, specific concepts. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa8cc4-e76e-4c42-88a0-055b7ed5eceb",
   "metadata": {},
   "source": [
    "Your model involves generating thousands of \"half-lived,\" \"truncated\" sentences over 8 epochs.\n",
    "\n",
    "Your model has \"summary production\" knowledge, but it has NEVER learned to \"finish the summary and stop\" (</s>).\n",
    "\n",
    "He produces a few meaningful words (which is why your BERTScore is high and your ROUGE is low).\n",
    "\n",
    "Then he panics because he doesn't know how to \"stop.\"\n",
    "\n",
    "Your Broken Fine-Tuning Task: You tried to teach this model to \"summarize\" for 8 epochs. BUT, because 99% of the labels were clipped without the </s> (stop) token, this summarization training failed and was \"broken.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "972ea9ef-0834-41a9-b2a1-593db3c98548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0>, Türkiye, resmî adıyla Türkiye Cumhuriyeti, Türkiye'nin en kalabalık şehri, ekonomik merkezidir. Türkiye, Türkiye\n"
     ]
    }
   ],
   "source": [
    "tr_output = pipe(\n",
    "    [text_tr],\n",
    "    max_new_tokens=30,  \n",
    "    \n",
    "    min_length=10,\n",
    "    num_beams=4,\n",
    "    truncation=True,\n",
    "    \n",
    "    no_repeat_ngram_size=3,  \n",
    "    early_stopping=True      \n",
    ")\n",
    "\n",
    "print(tr_output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d333e4c-6a12-4f41-987d-c426e62c9eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<extra_id_0> could be considered a subset of the broader technique of transfer learning. Fine-tuning is the process of adapting a'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_output = pipe(\n",
    "    [text_en],\n",
    "    max_new_tokens = 30,\n",
    "    truncation = True,\n",
    "    no_repeat_ngram_size = 3,\n",
    "    min_length = 10,\n",
    "    num_beams = 4\n",
    ")\n",
    "\n",
    "\n",
    "en_output[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014a8e7-20ec-4452-aa01-7017c8d2184c",
   "metadata": {},
   "source": [
    "# This is the error i got first but i fixed it.\n",
    "## I wanted to keep the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e00948d-e54c-4b60-9dd6-21f6065dd681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9458/2159210272.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "/home/gokhan/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1039' max='3114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1039/3114 1:00:24 < 2:00:52, 0.29 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [244/244 03:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39m args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:2316\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2315\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2323\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:2790\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2790\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2796\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:3221\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3219\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3221\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3222\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:3170\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3170\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3173\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:4489\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4486\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4488\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4489\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4490\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4499\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/trainer.py:4780\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4778\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4780\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4782\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4784\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m     10\u001b[0m predictions, labels \u001b[38;5;241m=\u001b[39m eval_preds\n\u001b[1;32m     12\u001b[0m labels[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m---> 14\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m decoded_labels \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(labels, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m [pred\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m decoded_preds]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3884\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3862\u001b[0m     sequences: Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3865\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3866\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3867\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3868\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3869\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;124;03m        `list[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3886\u001b[0m             seq,\n\u001b[1;32m   3887\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3888\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3889\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3890\u001b[0m         )\n\u001b[1;32m   3891\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3892\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3885\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3861\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3862\u001b[0m     sequences: Union[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3865\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3866\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3867\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3868\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3869\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;124;03m        `list[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3883\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3885\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3887\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3890\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3891\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3892\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3924\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3921\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3922\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3924\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:682\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    681\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 682\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    685\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    688\u001b[0m )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args= args,\n",
    "    train_dataset=total_ds['train'],\n",
    "    eval_dataset=total_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8e5a1c2-fc02-4ccb-94e7-86257c5399cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 20 16:57:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 575.65                 Driver Version: 577.03         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   43C    P8              6W /   70W |    2393MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           10177      C   /python3.10                           N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b347f818-c8fd-4198-ab23-900ae2d6e558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482db2b9-18a9-4326-9608-1d5af4be01ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch (GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
